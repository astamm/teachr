---
title: "Linear Regression with R"
tutorial:
  id: "f6487f97-7138-46a7-a0ec-91d523953a2e"
  version: 1.0
output: learnr::tutorial
runtime: shiny_prerendered
author: Aymeric Stamm
---

```{r setup, include=FALSE}
library(learnr)
library(testwhat)
library(magrittr)
library(ggplot2)

options(repos = "https://cloud.r-project.org")
tutorial_options(
  exercise.timelimit = 60,
  exercise.checker = testwhat::testwhat_learnr
)
knitr::opts_chunk$set(comment = NA)
```

## Setup

The goal is to propose and estimate a model for explaining a *response variable* $Y_i$ from a number of fixed *predictors* $x_{i1}, \dots, x_{ik}$.

Mathematically, the model reads:

$$ Y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik} + \varepsilon_i, \quad \mbox{with} \quad \mathbb{E}[\varepsilon_i] = 0 \quad \mbox{and} \quad \mathbb{V}\mbox{ar}[\varepsilon_i] = \sigma^2. $$

We can summarize the assumptions on which relies the linear regression model as follows:

1. The predictors are fixed. This means that we do not assume (or take into account the) intrinsic variability in the predictor values. The randomness of $Y_i$ all comes from the error term $\varepsilon_i$. In particular, it implies that considering a sample of $n$ *i.i.d.* random response variables $Y_1, \dots, Y_n$ boils down to assuming that $\varepsilon_1, \dots, \varepsilon_n$ are *i.i.d.*;
2. The error random variables $\varepsilon_i$ are centered and of constant variance. Combined with Assumption 1, this means that $\mathbb{E}[\boldsymbol{\varepsilon}] = \mathbf{0}$ and $\mathrm{Cov}[\boldsymbol{\varepsilon}] = \sigma^2 \mathbb{I}_n$;
3. [*optional*] Parametric hypothesis testing and confidence intervals further require the assumption of normality for the error vector, *i.e.* $\boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbb{I}_n)$.

Let $Y_1, \dots, Y_n$ be a sample of $n$ *i.i.d.* random response variables with associated observed values $y_1, \dots, y_n$. We define the so-called *design matrix* $\mathbb{X}$ of size $n \times (k + 1)$ with $x_{ij}$ at row $i$ and column $j$. This allows us to put the model into matrix form:

$$ \mathbf{y} = \mathbb{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}. $$

**Warning:** The model is called *linear* regression not because we necessarily look for a linear relationship between $Y$ and each $X_i$ but because it is linear in the $\boldsymbol{\beta}$ *coefficients*.

## Estimation

Estimating the model consequently boils down to estimating 

- the regression coefficients $\boldsymbol{\beta}$; and,
- the common error variance term $\sigma^2$.

This is usually done be minimizing the sum of squared differences between the observed and predicted responses:

$$ \mbox{SSD}(\boldsymbol{\beta}, \sigma^2; \mathbf{y}, \mathbb{X}) := (\mathbf{y} - \mathbb{X} \boldsymbol{\beta})^\top (\mathbf{y} - \mathbb{X} \boldsymbol{\beta}) = \sum_{i = 1}^n (y_i - (\beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik}))^2 .$$

This leads to the following **coefficient estimator**:

$$ \widehat{\boldsymbol{\beta}} := (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^\top \mathbf{Y}, $$

from which we can define the **fitted response**:

$$ \widehat{\mathbf{Y}} = \mathbb{X} \widehat{\boldsymbol{\beta}} = \mathbb{X} (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^\top \mathbf{Y} = \mathbb{H} \mathbf{Y}. $$

The matrix $\mathbb{H}$ is often called the *hat matrix* or *projection matrix* or *influence matrix*. The diagonal terms are such that $0 \le h_{ii} \le 1$. They are called the **leverage score** of each observation $(y_i, x_{i1}, \dots, x_{ik}$. Note that the leverage does not depend on $y_i$ at all but only on the predictor values. Also, with some abuse of notation, we can write:

$$ h_{ii} = \frac{\partial \widehat{Y_i}}{\partial Y_i}, $$

which illustrates the leverage measures the degree by which the $i$-*th* measured value influences the $i$-*th* fitted value.

An unbiased estimator of the constant variance term $\sigma^2$ is given by:

$$ \widehat{\sigma^2} := \frac{(\mathbf{Y} - \widehat{\mathbf{Y}})^\top (\mathbf{Y} - \widehat{\mathbf{Y}})}{n - k - 1}. $$

## Residuals

We can define the **residuals** of a fitted linear regression model as the difference between the observed and fitted response values:

$$ \mathbf{e} := \mathbf{y} - \widehat{\mathbf{y}} = (\mathbb{I}_n - \mathbb{H}) \mathbf{y}. $$

As a result, we still do have $\mathbb{\mathbf{e}} = \mathbf{0}$ but we no longer have residuals with constant variance because $\mathbb{V}\mbox{ar}[e_i] = \sigma^2 (1 - h_{ii})$ and residuals are not uncorrelated anymore since $\mbox{Cov}(e_i, e_j) = -h_{ij}$, for $i \ne j$.

**Standardized residuals** are a way of estimating the error for a particular data point which takes into account the leverage/influence of the point. These are sometimes called *internally studentized residuals*

$$ r_i := \frac{e_i}{s(e_i)} = \frac{e_i}{\sqrt{\widehat{\sigma^2} \left( 1 - h_{ii} \right)}}. $$

The motivation behind standardized residuals is that even though our model assumed homoscedasticity with an *i.i.d.* error term with fixed variance $\varepsilon \sim \mathcal{N}(0, \sigma^2)$, the distribution of the residuals $e_i$ cannot be i.i.d. because the sum of residuals is always *exactly* zero. In fact, $\mbox{Cov}(\mathbf{e}) = I - H$.

**Studentized residuals** for any given data point are calculated from a model fit to every other data point except the one in question. This is variously called the *externally studentized residuals*, *deleted residuals* or *jackknifed residuals*.

This sounds computationally difficult (it sounds like we would have to fit one new model for every point) but in fact there's a way to compute it from just the original model without refitting using the standardized residuals $r_i$:

$$ t_i := r_i \left( \frac{n - k - 2}{n - k - 1 - r_i^2} \right)^{1/2}, $$

where $n$ is the total number of observations and $k$ is the number of predictors used in the model.

The motivation behind studentized residuals comes from their use in outlier testing. If we suspect a point is an outlier, then it was not generated from the assumed model, by definition. Therefore it would be a mistake - a violation of assumptions - to include that outlier in the fitting of the model. Studentized residuals are widely used in practical outlier detection.

Studentized residuals also have the desirable property that for each data point, the residual follows a Student's t-distribution, if the normality assumption of the original regression model is met.

**Source:** (https://stats.stackexchange.com/questions/204708/is-studentized-residuals-v-s-standardized-residuals-in-lm-model)

## Cook's distance

Cook's distance $D_i$ of observation $i$ (for $i = 1, \dots, n$) is defined as a normalized version of the sum of squared differences between the fitted values obtained by including or excluding that observation:

$$ D_i := \frac{ \sum_{j = 1}^{n} \left( \widehat{y}_j - \widehat{y}_j^{(-i)} \right)^2}{\widehat{\sigma^2} (k+1)}, $$
where $\widehat{y}_j^{(-i)}$ is the fitted response value obtained when excluding observation $i$. Interestingly, it can be expressed in terms the leverage scores and the standardized residuals in a quite simple form:

$$ D_i = \frac{r_i^2}{k+1} \frac{h_{ii}}{1-h_{ii}}. $$

This last equation shows that there are two phenomena that affect the value of $D_i$:

- If observation $i$ is an **outlier**, in the sense that it has probably been drawn from another distribution with respect to the other observations, then $r_i$ should be excessively high, which tends to increase Cook's distance;
- If observation $i$ has a lot of **influence**, in the sense that its leverage score is high, then Cook's distance increases.

**Notes:** 

1. You can have cases in which an observation might have high influence but is not necessarily an outlier and therefore should be kept for analysis.
2. The reverse can happen as well. Some points with low influence (low leverage score) can be outliers (high residual value). In this case, we could be tempted to remove the observation because it violates our assumption.

Cook's distance does not help in the above two situations, but it does not really matter, because, in both cases, we can safely include the corresponding observation into our regression.

## Model diagnosis

### Toy examples

Let us generate two data sets: one for which things will go well and one for which things will go sideways. The following code produces the desired data sets and put them together into a tidy tibble:

```{r nl-data-gen}
set.seed(1234)

n <- 1000      # sample size
x <- seq(0, 100, length.out = n)
first_data <- 3 + 0.1 * x + rnorm(n, sd = 3)
second_data <- 3 + 0.1 * x + 10 * sin(x / 3) + rnorm(n, sd = 3)

full_data <- tibble::tibble(x, first_data, second_data) %>%
  tidyr::pivot_longer(cols = -x, values_to = "y")
```

**The most important and first thing you should do before any attempt to modeling is to take a look at your data.**

```{r nl-data-viz, fig.align='center'}
full_data %>%
  ggplot(aes(x, y, col = name)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_grid(cols = vars(name)) + 
  labs(
    title = "Scatterplots of y against x.",
    subtitle = "By dataset."
  ) + 
  theme_bw() + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(option = "cividis")
```

As you can see, both data sets seem to generate the same linear regression predictions, although we already clearly understand which one will go sideways...

Let us go deeper now. If one really want to fit a linear regression model to explain `y` in terms of `x` in both situations, you can use the function `lm()` that does all the computations for you:

```{r nl-data-mod}
mod1 <- lm(first_data ~ x)
mod1
mod2 <- lm(second_data ~ x)
mod2
```

Before going into the syntax and the content of the `lm()` function, remember that, **even though R facilitates computations, it does not verify for you that the assumptions required by the model are met by your data**!

This is where *model diagnosis* comes into play. You can almost entirely diagnose your model graphically. Using the grammar of graphics in `ggplot2`, this can be achieved using the extension [**lindia**](https://github.com/astamm/lindia/tree/fix-residuals), that you can also find by borwsing [ggplot extensions](http://www.ggplot2-exts.org/gallery/). In particular, the function `lindia::gg_diagnose()` shows the relevant plots for diagnosis all at once.

Let us diagnose the first model:

```{r diagnose-mod1, fig.align='center'}
lindia::gg_diagnose(fitted.lm = mod1, plot.all = FALSE)
```

*A note on Cook's distance threshold.* Some texts recommend to flag as outliers observations with a Cook's distance greater than $1$. Other textbooks use the theshold $4/n$. John Fox, in his booklet on regression diagnostics is rather cautious when it comes to giving numerical thresholds. He advises the use of graphics and to examine in closer details the points with

> values of D that are substantially larger than the rest (thresholds should just be used to enhance graphical displays), John Fox (1991), *Regression Diagnostics: An Introduction*, Sage Publications.

A theoretically justified approach can be achieved by looking at the distributional properties. If the error vector $\boldsymbol{\varepsilon}$ follows a multivariate Gassian distribution:

$$ \boldsymbol{\varepsilon} \sim \mathcal{N}_n(\mathbf{0}, \sigma^2 I_n), $$

then it can be [proved](https://stats.stackexchange.com/questions/400217/squared-internally-studentized-residual-over-n-p-is-beta-distributed) that each squared standardized residual follows a *Beta distribution*:

$$ \frac{r_i^2}{n - k - 1} \sim \mathcal{B}eta \left( \frac{1}{2}, \frac{n - k - 2}{2} \right). $$

Thus we end up with the following distribution for Cook's distance:

$$ \frac{k+1}{n - (k+1)} \frac{1 - h_{ii}}{h_{ii}} D_i \sim \mathcal{B}eta \left( \frac{1}{2}, \frac{n - k - 2}{2} \right). $$

As a result, in average, we should not observe more than $n_\mathrm{authorized} = \alpha n$ observations having a Cook's distance that exceeds the quantile of order $1 - \alpha$ of the above Beta distribution. Let $n_\mathrm{beta}$ be the number of observations with a Cook's distance that *effectively* exceeds the quantile of order $1 - \alpha$ of the above Beta distribution. We can finally flag as outliers those $n_\mathrm{beta} - n_\mathrm{authorized}$ observations with highest Cook's distance, if any.

```{r diagnose-mod1-cook, fig.align='center'}
lindia::gg_cooksd(fitted.lm = mod1, threshold = "theoretical")
```

Let us now diagnose the second model:

```{r diagnose-mod2, fig.align='center'}
lindia::gg_diagnose(fitted.lm = mod2, plot.all = FALSE)
```

```{r diagnose-mod2-cook, fig.align='center'}
lindia::gg_cooksd(fitted.lm = mod2, threshold = "theoretical")
```

## Box-Cox transformations and beyond

[topo](https://www.ime.usp.br/~abe/lista/pdfm9cJKUmFZp.pdf)

The purpose of transforming data is whenever your response random vector $\mathbf{Y}$ cannot be assumed to follow a Gaussian distribution. It is, in some cases, possible to find a *parametric* transformation $h_\lambda: \mathbb{R} \to \mathbb{R}$ such that $h_\lambda(\mathbf{Y}) \sim \mathcal{N}_n(\mathbb{X} \boldsymbol{\beta}, \sigma^2 \mathbb{I}_n)$.

Once you specify the parametric family of transformations to look at, you can then estimate the optimal parameter(s) for instance by maximizing the likelihood.

### Box-Cox Transformation

The first family of transformations was proposed by Box and Cox, in their 1964 seminal paper on the topic, which gave its name to the Box-Cox transformations:

$$ h_\lambda^\mathrm{BC}(y) = \begin{cases} \frac{(y + c)^\lambda - 1}{\lambda}, & \mbox{if } \lambda \ne 0 \\ \log(y + c), & \mbox{if } \lambda = 0, \end{cases} $$
where $c$ is a constant such that $y + c > 0$, for any $y$.

### Yeo-Johnson Transformation

Since then, there has been a lot of variants around this family of transformations. The Yeo-Johnson transformation is particularly popular because it has a clear interpretation: *it minimizes the Kullback-Leibler distance between the normal distribution and the transformed distribution*. The corresponding family of transformations reads:

$$ h_\lambda^\mathrm{YJ}(y) = \begin{cases} \frac{(y + 1)^\lambda - 1}{\lambda}, & \mbox{if } \lambda \ne 0, \quad y \ge 0, \\ \log(y + 1), & \mbox{if } \lambda = 0, \quad y \ge 0, \\ \frac{(1 - y)^{2 - \lambda} - 1}{\lambda - 2}, & \mbox{if } \lambda \ne 2, \quad y < 0, \\ -\log(1 - y), & \mbox{if } \lambda = 2, \quad y < 0. \end{cases} $$
It also presents the advantage of naturally accommodating negative values w.r.t. the Box-Cox transformation.

### Implementations in R

In the [**recipes**](https://tidymodels.github.io/recipes/index.html), check the functions [step_BoxCox](https://tidymodels.github.io/recipes/reference/step_BoxCox.html) and [step_YeoJohnson](https://tidymodels.github.io/recipes/reference/step_YeoJohnson.html)

## Building more complex models

### Model specification

```{r model-spec, eval=FALSE}
mod <- lm(y ~ I(log(co1)) + co2 * co3 + co4 * ca, data = fancy_data)
```

- the `I()` syntax,
- Interaction terms
- the `.` to select all vars, and `.^2` to include all interactions as well
- the equivalence `A * B` and `A + B + A:B`

### Understanding the output of `lm()`

### Exploring interactions

The [**interactions**](https://interactions.jacob-long.com) package, with [vignette](https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html).

### Testing linear hypotheses

https://www.r-bloggers.com/introducing-olsrr/

#### Multicollinearity

#### Model selection

Here we present the simple *forward* and *backward* procedures to model selection for linear regression models. This is implemented in the R package **stats** via the function `step()`.

- `direction = "backward"`. It starts with the full specified model. It removes one at a time predictors, by order of the predictors (n-way interactions first, then n-1, etc. to finish with main effects), using the AIC which combines fitting term (how well the model explains the data) and model complexity (how many parameters does the model require to estimate). The idea is that the more parameters you make your model depend on the better it will explain the data but, at some point, you will probably overfit the data. In addition, simpler models are easier to interpret and more practical.

- `direction = "forward"`. ?

- `direction = "both"`. ?

Note that there are many other ways to perform model selection. One [interesting and funny approach](https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.446) borrows ideas from co-operative game theory: it uses the Shapley–Shubik power index, which was formulated by Lloyd Shapley and Martin Shubik in 1954 to measure the powers of players in a voting game. In the context of regression analysis, it is used to measure the importance of predictors in explaining the response and it has been shown to provide consistent results in the presence of multicollinearity.

## Exercises
