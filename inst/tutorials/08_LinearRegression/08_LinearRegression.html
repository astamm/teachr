<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Aymeric Stamm" />


<meta name="progressive" content="false" />
<meta name="allow-skip" content="false" />

<title>Linear Regression with R</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->



</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<div id="section-setup" class="section level2">
<h2>Setup</h2>
<p>The goal is to propose and estimate a model for explaining a <em>response variable</em> <span class="math inline">\(Y_i\)</span> from a number of fixed <em>predictors</em> <span class="math inline">\(x_{i1}, \dots, x_{ik}\)</span>.</p>
<p>Mathematically, the model reads:</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik} + \varepsilon_i, \quad \mbox{with} \quad \mathbb{E}[\varepsilon_i] = 0 \quad \mbox{and} \quad \mathbb{V}\mbox{ar}[\varepsilon_i] = \sigma^2. \]</span></p>
<p>We can summarize the assumptions on which relies the linear regression model as follows:</p>
<ol style="list-style-type: decimal">
<li>The predictors are fixed. This means that we do not assume (or take into account the) intrinsic variability in the predictor values. The randomness of <span class="math inline">\(Y_i\)</span> all comes from the error term <span class="math inline">\(\varepsilon_i\)</span>. In particular, it implies that considering a sample of <span class="math inline">\(n\)</span> <em>i.i.d.</em> random response variables <span class="math inline">\(Y_1, \dots, Y_n\)</span> boils down to assuming that <span class="math inline">\(\varepsilon_1, \dots, \varepsilon_n\)</span> are <em>i.i.d.</em>;</li>
<li>The error random variables <span class="math inline">\(\varepsilon_i\)</span> are centered and of constant variance. Combined with Assumption 1, this means that <span class="math inline">\(\mathbb{E}[\boldsymbol{\varepsilon}] = \mathbf{0}\)</span> and <span class="math inline">\(\mathrm{Cov}[\boldsymbol{\varepsilon}] = \sigma^2 \mathbb{I}_n\)</span>;</li>
<li>[<em>optional</em>] Parametric hypothesis testing and confidence intervals further require the assumption of normality for the error vector, <em>i.e.</em> <span class="math inline">\(\boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbb{I}_n)\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(Y_1, \dots, Y_n\)</span> be a sample of <span class="math inline">\(n\)</span> <em>i.i.d.</em> random response variables with associated observed values <span class="math inline">\(y_1, \dots, y_n\)</span>. We define the so-called <em>design matrix</em> <span class="math inline">\(\mathbb{X}\)</span> of size <span class="math inline">\(n \times (k + 1)\)</span> with <span class="math inline">\(x_{ij}\)</span> at row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span>. This allows us to put the model into matrix form:</p>
<p><span class="math display">\[ \mathbf{y} = \mathbb{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}. \]</span></p>
<p><strong>Warning:</strong> The model is called <em>linear</em> regression not because we necessarily look for a linear relationship between <span class="math inline">\(Y\)</span> and each <span class="math inline">\(X_i\)</span> but because it is linear in the <span class="math inline">\(\boldsymbol{\beta}\)</span> <em>coefficients</em>.</p>
</div>
<div id="section-estimation" class="section level2">
<h2>Estimation</h2>
<p>Estimating the model consequently boils down to estimating</p>
<ul>
<li>the regression coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span>; and,</li>
<li>the common error variance term <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<p>This is usually done be minimizing the sum of squared differences between the observed and predicted responses:</p>
<p><span class="math display">\[ \mbox{SSD}(\boldsymbol{\beta}, \sigma^2; \mathbf{y}, \mathbb{X}) := (\mathbf{y} - \mathbb{X} \boldsymbol{\beta})^\top (\mathbf{y} - \mathbb{X} \boldsymbol{\beta}) = \sum_{i = 1}^n (y_i - (\beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik}))^2 .\]</span></p>
<p>This leads to the following <strong>coefficient estimator</strong>:</p>
<p><span class="math display">\[ \widehat{\boldsymbol{\beta}} := (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^\top \mathbf{Y}, \]</span></p>
<p>from which we can define the <strong>fitted response</strong>:</p>
<p><span class="math display">\[ \widehat{\mathbf{Y}} = \mathbb{X} \widehat{\boldsymbol{\beta}} = \mathbb{X} (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^\top \mathbf{Y} = \mathbb{H} \mathbf{Y}. \]</span></p>
<p>The matrix <span class="math inline">\(\mathbb{H}\)</span> is often called the <em>hat matrix</em> or <em>projection matrix</em> or <em>influence matrix</em>. The diagonal terms are such that <span class="math inline">\(0 \le h_{ii} \le 1\)</span>. They are called the <strong>leverage score</strong> of each observation <span class="math inline">\((y_i, x_{i1}, \dots, x_{ik}\)</span>. Note that the leverage does not depend on <span class="math inline">\(y_i\)</span> at all but only on the predictor values. Also, with some abuse of notation, we can write:</p>
<p><span class="math display">\[ h_{ii} = \frac{\partial \widehat{Y_i}}{\partial Y_i}, \]</span></p>
<p>which illustrates the leverage measures the degree by which the <span class="math inline">\(i\)</span>-<em>th</em> measured value influences the <span class="math inline">\(i\)</span>-<em>th</em> fitted value.</p>
<p>An unbiased estimator of the constant variance term <span class="math inline">\(\sigma^2\)</span> is given by:</p>
<p><span class="math display">\[ \widehat{\sigma^2} := \frac{(\mathbf{Y} - \widehat{\mathbf{Y}})^\top (\mathbf{Y} - \widehat{\mathbf{Y}})}{n - k - 1}. \]</span></p>
</div>
<div id="section-residuals" class="section level2">
<h2>Residuals</h2>
<p>We can define the <strong>residuals</strong> of a fitted linear regression model as the difference between the observed and fitted response values:</p>
<p><span class="math display">\[ \mathbf{e} := \mathbf{y} - \widehat{\mathbf{y}} = (\mathbb{I}_n - \mathbb{H}) \mathbf{y}. \]</span></p>
<p>As a result, we still do have <span class="math inline">\(\mathbb{\mathbf{e}} = \mathbf{0}\)</span> but we no longer have residuals with constant variance because <span class="math inline">\(\mathbb{V}\mbox{ar}[e_i] = \sigma^2 (1 - h_{ii})\)</span> and residuals are not uncorrelated anymore since <span class="math inline">\(\mbox{Cov}(e_i, e_j) = -h_{ij}\)</span>, for <span class="math inline">\(i \ne j\)</span>.</p>
<p><strong>Standardized residuals</strong> are a way of estimating the error for a particular data point which takes into account the leverage/influence of the point. These are sometimes called <em>internally studentized residuals</em></p>
<p><span class="math display">\[ r_i := \frac{e_i}{s(e_i)} = \frac{e_i}{\sqrt{\widehat{\sigma^2} \left( 1 - h_{ii} \right)}}. \]</span></p>
<p>The motivation behind standardized residuals is that even though our model assumed homoscedasticity with an <em>i.i.d.</em> error term with fixed variance <span class="math inline">\(\varepsilon \sim \mathcal{N}(0, \sigma^2)\)</span>, the distribution of the residuals <span class="math inline">\(e_i\)</span> cannot be i.i.d. because the sum of residuals is always <em>exactly</em> zero. In fact, <span class="math inline">\(\mbox{Cov}(\mathbf{e}) = I - H\)</span>.</p>
<p><strong>Studentized residuals</strong> for any given data point are calculated from a model fit to every other data point except the one in question. This is variously called the <em>externally studentized residuals</em>, <em>deleted residuals</em> or <em>jackknifed residuals</em>.</p>
<p>This sounds computationally difficult (it sounds like we would have to fit one new model for every point) but in fact there’s a way to compute it from just the original model without refitting using the standardized residuals <span class="math inline">\(r_i\)</span>:</p>
<p><span class="math display">\[ t_i := r_i \left( \frac{n - k - 2}{n - k - 1 - r_i^2} \right)^{1/2}, \]</span></p>
<p>where <span class="math inline">\(n\)</span> is the total number of observations and <span class="math inline">\(k\)</span> is the number of predictors used in the model.</p>
<p>The motivation behind studentized residuals comes from their use in outlier testing. If we suspect a point is an outlier, then it was not generated from the assumed model, by definition. Therefore it would be a mistake - a violation of assumptions - to include that outlier in the fitting of the model. Studentized residuals are widely used in practical outlier detection.</p>
<p>Studentized residuals also have the desirable property that for each data point, the residual follows a Student’s t-distribution, if the normality assumption of the original regression model is met.</p>
<p><strong>Source:</strong> (<a href="https://stats.stackexchange.com/questions/204708/is-studentized-residuals-v-s-standardized-residuals-in-lm-model" class="uri">https://stats.stackexchange.com/questions/204708/is-studentized-residuals-v-s-standardized-residuals-in-lm-model</a>)</p>
</div>
<div id="section-cooks-distance" class="section level2">
<h2>Cook’s distance</h2>
<p>Cook’s distance <span class="math inline">\(D_i\)</span> of observation <span class="math inline">\(i\)</span> (for <span class="math inline">\(i = 1, \dots, n\)</span>) is defined as a normalized version of the sum of squared differences between the fitted values obtained by including or excluding that observation:</p>
<p><span class="math display">\[ D_i := \frac{ \sum_{j = 1}^{n} \left( \widehat{y}_j - \widehat{y}_j^{(-i)} \right)^2}{\widehat{\sigma^2} (k+1)}, \]</span> where <span class="math inline">\(\widehat{y}_j^{(-i)}\)</span> is the fitted response value obtained when excluding observation <span class="math inline">\(i\)</span>. Interestingly, it can be expressed in terms the leverage scores and the standardized residuals in a quite simple form:</p>
<p><span class="math display">\[ D_i = \frac{r_i^2}{k+1} \frac{h_{ii}}{1-h_{ii}}. \]</span></p>
<p>This last equation shows that there are two phenomena that affect the value of <span class="math inline">\(D_i\)</span>:</p>
<ul>
<li>If observation <span class="math inline">\(i\)</span> is an <strong>outlier</strong>, in the sense that it has probably been drawn from another distribution with respect to the other observations, then <span class="math inline">\(r_i\)</span> should be excessively high, which tends to increase Cook’s distance;</li>
<li>If observation <span class="math inline">\(i\)</span> has a lot of <strong>influence</strong>, in the sense that its leverage score is high, then Cook’s distance increases.</li>
</ul>
<p><strong>Notes:</strong></p>
<ol style="list-style-type: decimal">
<li>You can have cases in which an observation might have high influence but is not necessarily an outlier and therefore should be kept for analysis.</li>
<li>The reverse can happen as well. Some points with low influence (low leverage score) can be outliers (high residual value). In this case, we could be tempted to remove the observation because it violates our assumption.</li>
</ol>
<p>Cook’s distance does not help in the above two situations, but it does not really matter, because, in both cases, we can safely include the corresponding observation into our regression.</p>
</div>
<div id="section-model-diagnosis" class="section level2">
<h2>Model diagnosis</h2>
<div id="section-toy-examples" class="section level3">
<h3>Toy examples</h3>
<p>Let us generate two data sets: one for which things will go well and one for which things will go sideways. The following code produces the desired data sets and put them together into a tidy tibble:</p>
<pre class="r"><code>set.seed(1234)

n &lt;- 1000      # sample size
x &lt;- seq(0, 100, length.out = n)
first_data &lt;- 3 + 0.1 * x + rnorm(n, sd = 3)
second_data &lt;- 3 + 0.1 * x + 10 * sin(x / 3) + rnorm(n, sd = 3)

full_data &lt;- tibble::tibble(x, first_data, second_data) %&gt;%
  tidyr::pivot_longer(cols = -x, values_to = &quot;y&quot;)</code></pre>
<p><strong>The most important and first thing you should do before any attempt to modeling is to take a look at your data.</strong></p>
<pre class="r"><code>full_data %&gt;%
  ggplot(aes(x, y, col = name)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  facet_grid(cols = vars(name)) + 
  labs(
    title = &quot;Scatterplots of y against x.&quot;,
    subtitle = &quot;By dataset.&quot;
  ) + 
  theme_bw() + 
  theme(legend.position = &quot;none&quot;) + 
  scale_color_viridis_d(option = &quot;cividis&quot;)</code></pre>
<p><img src="08_LinearRegression_files/figure-html/nl-data-viz-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>As you can see, both data sets seem to generate the same linear regression predictions, although we already clearly understand which one will go sideways…</p>
<p>Let us go deeper now. If one really want to fit a linear regression model to explain <code>y</code> in terms of <code>x</code> in both situations, you can use the function <code>lm()</code> that does all the computations for you:</p>
<pre class="r"><code>mod1 &lt;- lm(first_data ~ x)
mod1</code></pre>
<pre><code>
Call:
lm(formula = first_data ~ x)

Coefficients:
(Intercept)            x  
    2.94385      0.09953  </code></pre>
<pre class="r"><code>mod2 &lt;- lm(second_data ~ x)
mod2</code></pre>
<pre><code>
Call:
lm(formula = second_data ~ x)

Coefficients:
(Intercept)            x  
    3.90061      0.09098  </code></pre>
<p>Before going into the syntax and the content of the <code>lm()</code> function, remember that, <strong>even though R facilitates computations, it does not verify for you that the assumptions required by the model are met by your data</strong>!</p>
<p>This is where <em>model diagnosis</em> comes into play. You can almost entirely diagnose your model graphically. Using the grammar of graphics in <code>ggplot2</code>, this can be achieved using the extension <a href="https://github.com/astamm/lindia/tree/fix-residuals"><strong>lindia</strong></a>, that you can also find by borwsing <a href="http://www.ggplot2-exts.org/gallery/">ggplot extensions</a>. In particular, the function <code>lindia::gg_diagnose()</code> shows the relevant plots for diagnosis all at once.</p>
<p>Let us diagnose the first model:</p>
<pre class="r"><code>lindia::gg_diagnose(fitted.lm = mod1, plot.all = FALSE)</code></pre>
<pre><code>$residual_hist</code></pre>
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod1-1.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$x</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod1-2.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$res_fitted</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod1-3.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$qqplot</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod1-4.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$scalelocation</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod1-5.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$resleverage</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod1-6.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$cooksd</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod1-7.png" width="624" style="display: block; margin: auto;" /></p>
<p><em>A note on Cook’s distance threshold.</em> Some texts recommend to flag as outliers observations with a Cook’s distance greater than <span class="math inline">\(1\)</span>. Other textbooks use the theshold <span class="math inline">\(4/n\)</span>. John Fox, in his booklet on regression diagnostics is rather cautious when it comes to giving numerical thresholds. He advises the use of graphics and to examine in closer details the points with</p>
<blockquote>
<p>values of D that are substantially larger than the rest (thresholds should just be used to enhance graphical displays), John Fox (1991), <em>Regression Diagnostics: An Introduction</em>, Sage Publications.</p>
</blockquote>
<p>A theoretically justified approach can be achieved by looking at the distributional properties. If the error vector <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> follows a multivariate Gassian distribution:</p>
<p><span class="math display">\[ \boldsymbol{\varepsilon} \sim \mathcal{N}_n(\mathbf{0}, \sigma^2 I_n), \]</span></p>
<p>then it can be <a href="https://stats.stackexchange.com/questions/400217/squared-internally-studentized-residual-over-n-p-is-beta-distributed">proved</a> that each squared standardized residual follows a <em>Beta distribution</em>:</p>
<p><span class="math display">\[ \frac{r_i^2}{n - k - 1} \sim \mathcal{B}eta \left( \frac{1}{2}, \frac{n - k - 2}{2} \right). \]</span></p>
<p>Thus we end up with the following distribution for Cook’s distance:</p>
<p><span class="math display">\[ \frac{k+1}{n - (k+1)} \frac{1 - h_{ii}}{h_{ii}} D_i \sim \mathcal{B}eta \left( \frac{1}{2}, \frac{n - k - 2}{2} \right). \]</span></p>
<p>As a result, in average, we should not observe more than <span class="math inline">\(n_\mathrm{authorized} = \alpha n\)</span> observations having a Cook’s distance that exceeds the quantile of order <span class="math inline">\(1 - \alpha\)</span> of the above Beta distribution. Let <span class="math inline">\(n_\mathrm{beta}\)</span> be the number of observations with a Cook’s distance that <em>effectively</em> exceeds the quantile of order <span class="math inline">\(1 - \alpha\)</span> of the above Beta distribution. We can finally flag as outliers those <span class="math inline">\(n_\mathrm{beta} - n_\mathrm{authorized}\)</span> observations with highest Cook’s distance, if any.</p>
<pre class="r"><code>lindia::gg_cooksd(fitted.lm = mod1, threshold = &quot;theoretical&quot;)</code></pre>
<pre><code>Cut-off for outliers (0.0140431883408406) is larger than all Cook&#39;s distances. The cut-off line might therefore not appear on the plot.</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod1-cook-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>Let us now diagnose the second model:</p>
<pre class="r"><code>lindia::gg_diagnose(fitted.lm = mod2, plot.all = FALSE)</code></pre>
<pre><code>$residual_hist</code></pre>
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod2-1.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$x</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod2-2.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$res_fitted</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod2-3.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$qqplot</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod2-4.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$scalelocation</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod2-5.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$resleverage</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod2-6.png" width="624" style="display: block; margin: auto;" /></p>
<pre><code>
$cooksd</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod2-7.png" width="624" style="display: block; margin: auto;" /></p>
<pre class="r"><code>lindia::gg_cooksd(fitted.lm = mod2, threshold = &quot;theoretical&quot;)</code></pre>
<pre><code>Cut-off for outliers (0.00860348502803006) is larger than all Cook&#39;s distances. The cut-off line might therefore not appear on the plot.</code></pre>
<p><img src="08_LinearRegression_files/figure-html/diagnose-mod2-cook-1.png" width="624" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="section-box-cox-transformations-and-beyond" class="section level2">
<h2>Box-Cox transformations and beyond</h2>
<p><a href="https://www.ime.usp.br/~abe/lista/pdfm9cJKUmFZp.pdf">topo</a></p>
<p>The purpose of transforming data is whenever your response random vector <span class="math inline">\(\mathbf{Y}\)</span> cannot be assumed to follow a Gaussian distribution. It is, in some cases, possible to find a <em>parametric</em> transformation <span class="math inline">\(h_\lambda: \mathbb{R} \to \mathbb{R}\)</span> such that <span class="math inline">\(h_\lambda(\mathbf{Y}) \sim \mathcal{N}_n(\mathbb{X} \boldsymbol{\beta}, \sigma^2 \mathbb{I}_n)\)</span>.</p>
<p>Once you specify the parametric family of transformations to look at, you can then estimate the optimal parameter(s) for instance by maximizing the likelihood.</p>
<div id="section-box-cox-transformation" class="section level3">
<h3>Box-Cox Transformation</h3>
<p>The first family of transformations was proposed by Box and Cox, in their 1964 seminal paper on the topic, which gave its name to the Box-Cox transformations:</p>
<p><span class="math display">\[ h_\lambda^\mathrm{BC}(y) = \begin{cases} \frac{(y + c)^\lambda - 1}{\lambda}, &amp; \mbox{if } \lambda \ne 0 \\ \log(y + c), &amp; \mbox{if } \lambda = 0, \end{cases} \]</span> where <span class="math inline">\(c\)</span> is a constant such that <span class="math inline">\(y + c &gt; 0\)</span>, for any <span class="math inline">\(y\)</span>.</p>
</div>
<div id="section-yeo-johnson-transformation" class="section level3">
<h3>Yeo-Johnson Transformation</h3>
<p>Since then, there has been a lot of variants around this family of transformations. The Yeo-Johnson transformation is particularly popular because it has a clear interpretation: <em>it minimizes the Kullback-Leibler distance between the normal distribution and the transformed distribution</em>. The corresponding family of transformations reads:</p>
<p><span class="math display">\[ h_\lambda^\mathrm{YJ}(y) = \begin{cases} \frac{(y + 1)^\lambda - 1}{\lambda}, &amp; \mbox{if } \lambda \ne 0, \quad y \ge 0, \\ \log(y + 1), &amp; \mbox{if } \lambda = 0, \quad y \ge 0, \\ \frac{(1 - y)^{2 - \lambda} - 1}{\lambda - 2}, &amp; \mbox{if } \lambda \ne 2, \quad y &lt; 0, \\ -\log(1 - y), &amp; \mbox{if } \lambda = 2, \quad y &lt; 0. \end{cases} \]</span> It also presents the advantage of naturally accommodating negative values w.r.t. the Box-Cox transformation.</p>
</div>
<div id="section-implementations-in-r" class="section level3">
<h3>Implementations in R</h3>
<p>In the <a href="https://tidymodels.github.io/recipes/index.html"><strong>recipes</strong></a>, check the functions <a href="https://tidymodels.github.io/recipes/reference/step_BoxCox.html">step_BoxCox</a> and <a href="https://tidymodels.github.io/recipes/reference/step_YeoJohnson.html">step_YeoJohnson</a></p>
</div>
</div>
<div id="section-building-more-complex-models" class="section level2">
<h2>Building more complex models</h2>
<div id="section-model-specification" class="section level3">
<h3>Model specification</h3>
<pre class="r"><code>mod &lt;- lm(y ~ I(log(co1)) + co2 * co3 + co4 * ca, data = fancy_data)</code></pre>
<ul>
<li>the <code>I()</code> syntax,</li>
<li>Interaction terms</li>
<li>the <code>.</code> to select all vars, and <code>.^2</code> to include all interactions as well</li>
<li>the equivalence <code>A * B</code> and <code>A + B + A:B</code></li>
</ul>
</div>
<div id="section-understanding-the-output-of-lm" class="section level3">
<h3>Understanding the output of <code>lm()</code></h3>
</div>
<div id="section-exploring-interactions" class="section level3">
<h3>Exploring interactions</h3>
<p>The <a href="https://interactions.jacob-long.com"><strong>interactions</strong></a> package, with <a href="https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html">vignette</a>.</p>
</div>
<div id="section-testing-linear-hypotheses" class="section level3">
<h3>Testing linear hypotheses</h3>
<p><a href="https://www.r-bloggers.com/introducing-olsrr/" class="uri">https://www.r-bloggers.com/introducing-olsrr/</a></p>
<div id="section-multicollinearity" class="section level4">
<h4>Multicollinearity</h4>
</div>
<div id="section-model-selection" class="section level4">
<h4>Model selection</h4>
<p>Here we present the simple <em>forward</em> and <em>backward</em> procedures to model selection for linear regression models. This is implemented in the R package <strong>stats</strong> via the function <code>step()</code>.</p>
<ul>
<li><p><code>direction = &quot;backward&quot;</code>. It starts with the full specified model. It removes one at a time predictors, by order of the predictors (n-way interactions first, then n-1, etc. to finish with main effects), using the AIC which combines fitting term (how well the model explains the data) and model complexity (how many parameters does the model require to estimate). The idea is that the more parameters you make your model depend on the better it will explain the data but, at some point, you will probably overfit the data. In addition, simpler models are easier to interpret and more practical.</p></li>
<li><p><code>direction = &quot;forward&quot;</code>. ?</p></li>
<li><p><code>direction = &quot;both&quot;</code>. ?</p></li>
</ul>
<p>Note that there are many other ways to perform model selection. One <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.446">interesting and funny approach</a> borrows ideas from co-operative game theory: it uses the Shapley–Shubik power index, which was formulated by Lloyd Shapley and Martin Shubik in 1954 to measure the powers of players in a voting game. In the context of regression analysis, it is used to measure the importance of predictors in explaining the response and it has been shown to provide consistent results in the presence of multicollinearity.</p>
</div>
</div>
</div>
<div id="section-exercises" class="section level2">
<h2>Exercises</h2>

<script type="application/shiny-prerendered" data-context="server-start">
library(learnr)
library(testwhat)
library(magrittr)
library(ggplot2)

options(repos = "https://cloud.r-project.org")
tutorial_options(
  exercise.timelimit = 60,
  exercise.checker = testwhat::testwhat_learnr
)
knitr::opts_chunk$set(comment = NA)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = list(id = "f6487f97-7138-46a7-a0ec-91d523953a2e", version = 1))
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.18"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/cerulean.min.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.18"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.18"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.18"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css","rstudio-theme.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.18"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.18"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.18"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.18"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.18"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["idb-keyvalue"]},{"type":"character","attributes":{},"value":["3.2.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/idb-keyval"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["idb-keyval-iife-compat.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.10.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.0"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68]}},"value":[{"type":"character","attributes":{},"value":["assertthat","backports","base","colorspace","compiler","crayon","datasets","digest","dplyr","evaluate","farver","fastmap","ggplot2","glue","graphics","grDevices","grid","gridExtra","gtable","htmltools","htmlwidgets","httpuv","jsonlite","knitr","labeling","later","lazyeval","learnr","lifecycle","lindia","magrittr","markdown","MASS","methods","mime","munsell","parallel","pillar","pkgconfig","plyr","praise","promises","purrr","R6","Rcpp","reshape2","rlang","rmarkdown","rprojroot","scales","shiny","stats","stringdist","stringi","stringr","testwhat","tibble","tidyr","tidyselect","tools","utils","vctrs","viridisLite","withr","xfun","xtable","yaml","zeallot"]},{"type":"character","attributes":{},"value":["0.2.1","1.1.5","3.6.1","1.4-1","3.6.1","1.3.4","3.6.1","0.6.23","0.8.3","0.14","2.0.1","1.0.1","3.2.1","1.3.1","3.6.1","3.6.1","3.6.1","2.3","0.3.0","0.4.0","1.5.1","1.5.2","1.6","1.26","0.3","1.0.0","0.2.2","0.10.0","0.1.0","0.9","1.5","1.1","7.3-51.4","3.6.1","0.7","0.5.0","3.6.1","1.4.2","2.0.3","1.8.4","1.0.0","1.1.0","0.3.3","2.4.1","1.0.3","1.4.3","0.4.2","1.18","1.3-2","1.1.0","1.4.0","3.6.1","0.9.5.5","1.4.3","1.4.0","4.11.1","2.1.3","1.0.0","0.2.5","3.6.1","3.6.1","0.2.0","0.3.0","2.1.2","0.11","1.8-4","2.2.0","0.1.0"]}]}]}
</script>
<!--/html_preserve-->
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">Linear Regression with R</h2>
<h4 class="author"><em>Aymeric Stamm</em></h4>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
